{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6e1278-a5e1-4fac-9adc-95f9d6c0f066",
   "metadata": {},
   "source": [
    "Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a8cd1-4ecc-4a19-962b-26258612ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -m pip install matplotlib tensorflow scikit-learn opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a819cf5b-aea1-46fe-ad41-493270782289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76d325-72c1-4cf9-ab96-0d1aaed58d5b",
   "metadata": {},
   "source": [
    "Keypoints using MP Holistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a478b9-adb4-40b5-9dcf-0b9ea48778fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57282205-3e8d-4013-9bc0-389ec06126a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion bgr/rgb\n",
    "    image.flags.writeable = False                  # non writeable image\n",
    "    results = model.process(image)                 # make prediction\n",
    "    image.flags.writeable = True                   # writable image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion rgb/bgr\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d157c0-7e29-4de9-a6c5-e91561bec449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12aff26a-33c2-46d8-a020-dcec56eaa555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "      ret, frame = cap.read()                #read feed\n",
    "      image, results = mediapipe_detection(frame, holistic) \n",
    "      print(results)\n",
    "      draw_styled_landmarks(image, results)         #draw landmarks\n",
    "      cv2.imshow('OpenCV Feed', image)       #show to screen\n",
    "      if cv2.waitKey(10) & 0xFF == ord('q'): #break gracefully\n",
    "        break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2d0f2-8492-4155-87c8-90bf6979d091",
   "metadata": {},
   "source": [
    "Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa90139-c46b-43fc-8e87-120cb920e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee34784-2c4c-4edd-a01d-f25a5115ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "# face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a37daae-56ef-4075-9993-2300047b8086",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3987811915.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    if results.face_landmarks\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() \n",
    "    if results.face_landmarks \n",
    "    else np.zeros(1404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95159c4c-f88c-48a4-a76a-aa0c3bdba131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh]) #([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa32c326-6549-4626-954c-c915b05b189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c3a83-9211-42b0-b5ba-482c4130ed6c",
   "metadata": {},
   "source": [
    "Collection Folder Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5acf9d2-4045-4767-9e4b-e18117daa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the folder names in the DATA_PATH\n",
    "# folders = os.listdir(actions)\n",
    "\n",
    "# # Add the folder names to the actions array\n",
    "# actions = np.concatenate((actions, folders))\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Hello', 'Goodbye', 'Good morning', 'Good night', 'I', 'You', 'He', 'She', 'It', 'We', 'They',\n",
    "                    'Who', 'What', 'Where', 'When', 'Why', 'How', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                    'Eat', 'Drink', 'Go', 'Come', 'Run','Walk', 'See', 'Hear',\n",
    "                    'Speak', 'Sleep','Man', 'Woman','I love you', 'Thank you'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de3277de-e3fc-4a98-8edf-b0e7bfa1c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for each action\n",
    "for action in actions:\n",
    "    action_dir = os.path.join(DATA_PATH, action)\n",
    "    if not os.path.exists(action_dir):\n",
    "        os.makedirs(action_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d34c989-cffe-4edd-8791-b39857c67a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hello', 'Goodbye', 'Good morning', 'Good night', 'I', 'You', 'He',\n",
       "       'She', 'It', 'We', 'They', 'Who', 'What', 'Where', 'When', 'Why',\n",
       "       'How', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'Eat',\n",
       "       'Drink', 'Go', 'Come', 'Run', 'Walk', 'See', 'Hear', 'Speak',\n",
       "       'Sleep', 'Man', 'Woman', 'I love you', 'Thank you'], dtype='<U12')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fcbc2f5-9b18-4770-9c09-cafc376b328e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4487400"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*30*3*1662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0b1e62-8372-45db-b750-5c43d8973282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello\n",
    "## 0\n",
    "## 1\n",
    "## 2\n",
    "## ...\n",
    "## 29\n",
    "# thanks\n",
    "\n",
    "# I love you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bcdff30-4238-4f8b-8132-cee19809d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5f871-23a7-49d4-903e-31326c94ffb3",
   "metadata": {},
   "source": [
    "Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b5e5c-a41d-4adf-a8b0-439a4b73c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                # print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "        # Wait for a key press to proceed to the next action\n",
    "        print(\"Press 'n' to proceed to the next action or 'q' to quit\")\n",
    "        while True:\n",
    "            if cv2.waitKey(0) & 0xFF == ord('n'):\n",
    "                break\n",
    "            elif cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1ab71-49ba-47fc-9d76-8af7bcc40021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set mediapipe model \n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "#     # NEW LOOP\n",
    "#     # Loop through actions\n",
    "#     for action in actions:\n",
    "#         # Get the list of frames for this action\n",
    "#         frame_list = os.listdir(os.path.join(DATA_PATH, action))\n",
    "        \n",
    "#         # Loop through frames\n",
    "#         for frame_name in frame_list:\n",
    "#             # Read image from folder\n",
    "#             frame = cv2.imread(os.path.join(DATA_PATH, action, frame_name))\n",
    "\n",
    "#             # Make detections\n",
    "#             image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "#             # Draw landmarks\n",
    "#             draw_styled_landmarks(image, results)\n",
    "                \n",
    "#             # Apply wait logic\n",
    "#             cv2.putText(image, 'Collecting frames for {}'.format(action), (15,12), \n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#             # Show to screen\n",
    "#             cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "#             # Export keypoints\n",
    "#             keypoints = extract_keypoints(results)\n",
    "#             npy_path = os.path.join('D:\\Jupyter Notebooks\\Convey\\MP_Data', action, frame_name.split('.')[0])  # replace 'New_Directory' with your desired directory\n",
    "#             os.makedirs(os.path.dirname(npy_path), exist_ok=True)  # create the directory if it doesn't exist\n",
    "#             np.save(npy_path, keypoints)\n",
    "\n",
    "#             # Break gracefully\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "                    \n",
    "#     cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a478d-7836-4e3c-b465-f7c05b5125c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the directory where the .npy files are\n",
    "# data_dir = r'D:\\Jupyter Notebooks\\Convey\\MP_Data'  # replace with your directory path\n",
    "\n",
    "# # Loop through all actions in the directory\n",
    "# for action in os.listdir(data_dir):\n",
    "#     action_dir = os.path.join(data_dir, action)\n",
    "    \n",
    "#     # Check if it's a directory\n",
    "#     if os.path.isdir(action_dir):\n",
    "#         npy_files = [f for f in os.listdir(action_dir) if f.endswith('.npy')]\n",
    "        \n",
    "#         # Sort the files\n",
    "#         npy_files.sort()\n",
    "        \n",
    "#         # Create a new subdirectory named '0'\n",
    "#         new_subdir = os.path.join(action_dir, '0')\n",
    "#         os.makedirs(new_subdir, exist_ok=True)\n",
    "        \n",
    "#         # Loop through all .npy files and rename them\n",
    "#         for i, filename in enumerate(npy_files, start=0):\n",
    "#             old_file = os.path.join(action_dir, filename)\n",
    "#             new_file = os.path.join(new_subdir, f\"{i}.npy\")\n",
    "            \n",
    "#             # Move and rename the file\n",
    "#             os.rename(old_file, new_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131fa68-c8dc-4e83-a497-566f991631e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6480c63-c544-4aa1-b298-496d8af712f0",
   "metadata": {},
   "source": [
    "Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82ab23c-a6e7-452e-a3c9-e0b653595191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e2c3d0b-7231-4801-8b4f-5fc8d9eeb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46479840-c9cf-4ace-acbf-afb7632f4ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 0,\n",
       " 'Goodbye': 1,\n",
       " 'Good morning': 2,\n",
       " 'Good night': 3,\n",
       " 'I': 4,\n",
       " 'You': 5,\n",
       " 'He': 6,\n",
       " 'She': 7,\n",
       " 'It': 8,\n",
       " 'We': 9,\n",
       " 'They': 10,\n",
       " 'Who': 11,\n",
       " 'What': 12,\n",
       " 'Where': 13,\n",
       " 'When': 14,\n",
       " 'Why': 15,\n",
       " 'How': 16,\n",
       " '1': 17,\n",
       " '2': 18,\n",
       " '3': 19,\n",
       " '4': 20,\n",
       " '5': 21,\n",
       " '6': 22,\n",
       " '7': 23,\n",
       " '8': 24,\n",
       " '9': 25,\n",
       " '10': 26,\n",
       " 'Eat': 27,\n",
       " 'Drink': 28,\n",
       " 'Go': 29,\n",
       " 'Come': 30,\n",
       " 'Run': 31,\n",
       " 'Walk': 32,\n",
       " 'See': 33,\n",
       " 'Hear': 34,\n",
       " 'Speak': 35,\n",
       " 'Sleep': 36,\n",
       " 'Man': 37,\n",
       " 'Woman': 38,\n",
       " 'I love you': 39,\n",
       " 'Thank you': 40}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8da82b-2d3a-4c20-a96b-027df9973f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff1c6a06-a7c4-445d-8c79-f465390bdfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.74513829,  0.35297501, -0.88064605, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.55038613,  0.34408551, -0.77222216, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.53582907,  0.33724731, -0.77980816, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.52738965,  0.34510583, -1.09216142, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.52749062,  0.34420687, -1.08049297, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.52763057,  0.34357068, -1.0707705 , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.52759242,  0.34168294, -1.07784915, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.55771673,  0.33277193, -0.70949394, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.57182765,  0.32888645, -0.65019679, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.45213476,  0.32967472, -0.96152037, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.44969472,  0.33006993, -0.94711286, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.44805431,  0.33067498, -0.96921408, ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.44694299,  0.33137396, -0.95132858, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.44755667,  0.33754915, -0.72686225, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.44799381,  0.34066087, -0.73904973, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.46194801,  0.35931486, -0.7663902 , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.46090156,  0.35905239, -0.68664491, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.45872998,  0.3575663 , -0.74035132, ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.52959144,  0.40041447, -0.59396213, ...,  0.51621318,\n",
       "          0.51909065, -0.03290127],\n",
       "        [ 0.53019607,  0.31825942, -0.77492517, ...,  0.51144403,\n",
       "          0.49898428, -0.06024634],\n",
       "        [ 0.53056175,  0.30197266, -0.85062635, ...,  0.51200491,\n",
       "          0.49692246, -0.0624674 ],\n",
       "        ...,\n",
       "        [ 0.55534828,  0.25557372, -0.965532  , ...,  0.52550024,\n",
       "          0.415407  , -0.04418607],\n",
       "        [ 0.55524272,  0.25565568, -0.95747578, ...,  0.5220558 ,\n",
       "          0.42335385, -0.05033704],\n",
       "        [ 0.55477738,  0.25816521, -0.93448573, ...,  0.52794117,\n",
       "          0.40437463, -0.0408388 ]],\n",
       "\n",
       "       [[ 0.55360407,  0.25853157, -0.88337183, ...,  0.52978498,\n",
       "          0.4073458 , -0.05053642],\n",
       "        [ 0.56014955,  0.26109669, -0.8471626 , ...,  0.53522217,\n",
       "          0.45961052, -0.04456182],\n",
       "        [ 0.56320798,  0.26256615, -0.85124815, ...,  0.52929705,\n",
       "          0.44093519, -0.00458901],\n",
       "        ...,\n",
       "        [ 0.56626517,  0.27496615, -0.93213212, ...,  0.53806686,\n",
       "          0.43636298, -0.01067813],\n",
       "        [ 0.56602204,  0.27518314, -0.86793917, ...,  0.53346324,\n",
       "          0.43460256, -0.00499244],\n",
       "        [ 0.56495905,  0.27534997, -0.84586895, ...,  0.53431571,\n",
       "          0.43573987, -0.00584141]],\n",
       "\n",
       "       [[ 0.56381154,  0.27550548, -0.84979141, ...,  0.53896934,\n",
       "          0.4319014 , -0.00599477],\n",
       "        [ 0.55910665,  0.35637   , -0.84378356, ...,  0.53303587,\n",
       "          0.59045041, -0.04508134],\n",
       "        [ 0.55873048,  0.37077704, -0.95209581, ...,  0.53110385,\n",
       "          0.60452759, -0.0546512 ],\n",
       "        ...,\n",
       "        [ 0.55159676,  0.37950104, -1.47568226, ...,  0.53016174,\n",
       "          0.60757548, -0.06975842],\n",
       "        [ 0.55170381,  0.37939361, -1.48425114, ...,  0.53123188,\n",
       "          0.60576332, -0.06947441],\n",
       "        [ 0.55176377,  0.37933409, -1.54058528, ...,  0.533319  ,\n",
       "          0.603383  , -0.06764941]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e577662-9c9b-420c-80f9-e1f46d1d4e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b30831fd-7caa-4fe8-9051-66de483ede04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "806067de-f3ed-42f3-9dc1-909ebd9c23a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230, 30, 258)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0254f5f-86b8-4e81-9490-a27244025302",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3959ce37-b256-474f-9525-7da6cef41870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e057156c-372b-4735-ba0c-8198f1bd066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfe2f77a-caf2-4567-b395-6b441504548d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 41)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32175f5-2d83-40ca-8db9-32aebb9da2f5",
   "metadata": {},
   "source": [
    "Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4717246-34c5-4bf4-b06c-b6cc08624298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4afe8b4b-32bc-4702-ad90-beacc4220204",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bb91da1-a0c6-4c02-9605-bf2b09b0015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258))) #1662\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443ca6f3-8e90-48e7-854f-a8415bcc2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.7, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45e5a256-a392-448e-8808-60f36f08991f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06f9d869-9e9a-4d32-a493-4d25fbccfdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba1cab60-7b8b-4804-ad95-b5b4f1338ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 2s 49ms/step - loss: 0.2394 - categorical_accuracy: 0.9195\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.4076 - categorical_accuracy: 0.8630\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 2s 48ms/step - loss: 0.3008 - categorical_accuracy: 0.8938\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.2277 - categorical_accuracy: 0.9264\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.1414 - categorical_accuracy: 0.9580\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 2s 49ms/step - loss: 0.2439 - categorical_accuracy: 0.9170\n",
      "Epoch 7/10\n",
      " 8/37 [=====>........................] - ETA: 1s - loss: 0.2309 - categorical_accuracy: 0.9180"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72848b44-25ba-4ef3-9445-624d28b9d2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            82688     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 41)                1353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238505 (931.66 KB)\n",
      "Trainable params: 238505 (931.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5f420-2524-443d-be2f-977334bd0413",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0df00529-2d1d-41aa-a9a5-66eb8677b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c327e9c-e351-499b-9d9a-79fc89af3afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2426fc6f-b487-4608-8aad-20157c7c694c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5d679-59c8-43f1-854c-8f71f2e07126",
   "metadata": {},
   "source": [
    "Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70790d4e-bc42-4adf-bd0b-ce52f3541393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('40_actions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf645381-cc1f-4be5-b4c5-7884e98ccf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e2dafa5-cd49-4428-8916-d3f0ba5f53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('40_actions.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0319d35-2dfb-4c4b-a9c9-277d2f46923e",
   "metadata": {},
   "source": [
    "Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf370a82-af62-48b4-a3f9-61290f821571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ab58ebb-9c4c-436a-89ba-5bcaccbc8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7329e97-bc1b-4d61-8975-a5e3bfb9779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44ab39f3-ce6c-42bd-ad36-03beec3dd1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[60,  1],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[61,  1],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[59,  1],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[59,  1],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[61,  1],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[57,  0],\n",
       "        [ 3,  2]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[58,  4],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[57,  1],\n",
       "        [ 4,  0]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[60,  1],\n",
       "        [ 1,  0]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[59,  0],\n",
       "        [ 1,  2]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[59,  1],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[60,  2],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[61,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[58,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[57,  2],\n",
       "        [ 1,  2]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[59,  0],\n",
       "        [ 2,  1]],\n",
       "\n",
       "       [[60,  1],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[60,  0],\n",
       "        [ 0,  2]]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9931b669-e855-4883-a3ff-d997d8332df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258064516129032"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b8d77-b7b6-40c6-9c7a-7f81f93f1fae",
   "metadata": {},
   "source": [
    "Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91080d-a5a7-482a-920f-4a3d5cab6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e91332-c3b9-4393-aaac-55b273607d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b8cc6-438b-4329-95f2-766bfb8047da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e08d6-0c80-46d6-b923-c4b9b3f26b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe7054-a7ce-457d-92de-4d5e79d930d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edaca4-7c81-4ec4-85e6-bf6b2c86cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.append('def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0c401-e27f-4e1d-967c-dff9c0346128",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10addde6-aced-422b-80a4-a7a81d607c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b50fbe-5e7b-44ae-a919-80c18357f2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.9\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 3: \n",
    "                sentence = sentence[-3:]\n",
    "\n",
    "            # # Viz probabilities\n",
    "            # image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688365b2-bf3e-4fd1-a450-3f9e3f7e7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660e76b-cd4d-4555-8439-f2e18e2eac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33680a-e674-4663-b507-0fd151ad100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_sequences,30,1662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f2e85-4816-45d1-a77e-8d618b2de4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(X_test[0], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ce5a6-ca09-4eec-a43f-81a28b95dfed",
   "metadata": {},
   "source": [
    "RERUN ONLY BELOW CELLS FOR MODEL LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163039a5-3a42-4226-956c-11ee3090eaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "mp_holistic = mp.solutions.holistic #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion bgr/rgb\n",
    "    image.flags.writeable = False                  # non writeable image\n",
    "    results = model.process(image)                 # make prediction\n",
    "    image.flags.writeable = True                   # writable image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion rgb/bgr\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf8a590-76ec-4d29-a9ff-09a8a12a8ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "      ret, frame = cap.read()                #read feed\n",
    "      image, results = mediapipe_detection(frame, holistic) \n",
    "      print(results)\n",
    "      draw_styled_landmarks(image, results)         #draw landmarks\n",
    "      cv2.imshow('OpenCV Feed', image)       #show to screen\n",
    "      if cv2.waitKey(10) & 0xFF == ord('q'): #break gracefully\n",
    "        break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0242b-c14f-4ec8-be34-b51e465f697a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\avani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2/2 [==============================] - 1s 28ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\n",
    "\n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "# face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh]) #([pose, face, lh, rh])\n",
    "\n",
    "result_test = extract_keypoints(results)\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Hello', 'Goodbye', 'Good morning', 'Good night', 'I', 'You', 'He', 'She', 'It', 'We', 'They',\n",
    "                    'Who', 'What', 'Where', 'When', 'Why', 'How', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                    'Eat', 'Drink', 'Go', 'Come', 'Run','Walk', 'See', 'Hear',\n",
    "                    'Speak', 'Sleep','Man', 'Woman','I love you', 'Thank you'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "\n",
    "X.shape\n",
    "\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258))) #1662\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "\n",
    "res = [.7, 0.2, 0.1]\n",
    "\n",
    "actions[np.argmax(res)]\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.load_weights('40_actions.h5')\n",
    "\n",
    "res = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.95\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.8, min_tracking_confidence=0.8) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()                #read feed\n",
    "        image, results = mediapipe_detection(frame, holistic) \n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 3: \n",
    "                sentence = sentence[-3:]\n",
    "\n",
    "            # # Viz probabilities\n",
    "            # image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4d175-bf73-4784-aba8-c045e9eb4452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
